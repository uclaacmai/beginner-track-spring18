{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting_House_Prices.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "L_h2o5nY0kBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting House Prices\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Street_scape_in_Boston-Edison.jpg\" width=400/>\n",
        "\n",
        "Today we are going to building a **linear regression** model to predict house prices in Boston.\n",
        "\n",
        "#### Review: Unsupervised vs Supervised Learning\n",
        "*   Supervised learning uses existing inputs and outputs to create a mapping function from inputs to outputs\n",
        "  * Can be used for regression problems, where outputs are continuous, or classification problems, where output is discrete\n",
        "*   Unsupervised learning finds relationships and clusters in input data\n",
        "\n",
        "We use the hypothesis function\n",
        "\n",
        "$$h(x^{i}) = w^{T}x^{i} + b$$\n",
        "\n",
        "to make an output prediction for each training example, representing the expected price of a house. We use the following loss function to calculate how accurate our prediction is:\n",
        "\n",
        "$$Loss = L(y^{i} , a^{i}) = \\frac{1}{2}(a^{i} - y^{i})^{2}$$\n",
        "\n",
        "Then, we apply gradient descent to optimize our weights.\n",
        "\n",
        "#### Some of the things you will learn\n",
        "* General architecture of a learning algorithm\n",
        "* initializing parameters\n",
        "* doing forward propagation\n",
        "* calculating the cost function\n",
        "* doing back propagation with gradient descent\n",
        "\n",
        "#### Instructions\n",
        "1. You will write your code between \n",
        "```python \n",
        "### START CODE HERE ###\n",
        "```\n",
        "```python \n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n",
        "2. Do not use any loops (the necessary loop is included for you!)\n",
        "\n",
        "## Packages\n",
        "First let's import some libraries\n",
        "* [scikit-learn](http://scikit-learn.org/stable/) is used to get our dataset\n",
        "* [numpy](http://www.numpy.org/) is used for scientific computing\n",
        "* [matplotlib](https://matplotlib.org/) is used for plotting data\n",
        "* [pandas](https://pandas.pydata.org/) is used for data processing and analysis\n",
        "* [seaborn](https://seaborn.pydata.org/) is used for statistical data visualization"
      ]
    },
    {
      "metadata": {
        "id": "wz1ekdOW0uZV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDhtIYE4cyXQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "metadata": {
        "id": "fKiD7zcJc9z8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "boston = load_boston()\n",
        "bos = pd.DataFrame(boston.data)\n",
        "bos.columns = boston.feature_names\n",
        "bos = bos.drop('B', axis = 1)\n",
        "bos['PRICE'] = boston.target\n",
        "pd.options.display.width = 300\n",
        "print(bos.head())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8-AxRNvCTtC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "         CRIM      per capita crime rate by town\n",
        "         ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "         INDUS     proportion of non-retail business acres per town\n",
        "         CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "         NOX       nitric oxides concentration (parts per 10 million)\n",
        "         RM        average number of rooms per dwelling\n",
        "         AGE       proportion of owner-occupied units built prior to 1940\n",
        "         DIS       weighted distances to five Boston employment centres\n",
        "         RAD       index of accessibility to radial highways\n",
        "         TAX       full-value property-tax rate per $10,000\n",
        "     PTRATIO   pupil-teacher ratio by town\n",
        "     LSTAT     % lower status of the population\n",
        "     MEDV      Median value of owner-occupied homes in $1000's"
      ]
    },
    {
      "metadata": {
        "id": "Q70FBBdf84tR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##"
      ]
    },
    {
      "metadata": {
        "id": "3BSgGkVtGBpa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# See the general trend in the prices\n",
        "\n",
        "sns.distplot(bos['PRICE'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ck-Ofh0I6dr8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plotting simple relations\n",
        "# Relation between Crime rate and Price:\n",
        "\n",
        "plt.plot(bos.iloc[::10, :]['PRICE'].sort_values(), bos.iloc[::10, :].sort_values('PRICE')['CRIM']*100)\n",
        "plt.xlabel('House price in $1000s')\n",
        "plt.ylabel('Crime Rate')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-77RXyFTD6_8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Relation between Age of the house and Price\n",
        "plt.plot(bos.iloc[::20, :]['PRICE'].sort_values(), bos.iloc[::20, :].sort_values('PRICE')['AGE'])\n",
        "plt.xlabel('House price in $1000s')\n",
        "plt.ylabel('Age')\n",
        "plt.title('Age Vs. Price')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CIxlsLT9GhIj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Pairwise Correlation between the features\n",
        "\n",
        "\n",
        "current_palette = sns.color_palette()\n",
        "corr=bos.iloc[:,[0,5,6,8,10,12]].corr()\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "sns.heatmap(corr, vmax=.8, linewidths=0.01, square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\n",
        "plt.title('Correlation between features');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "im1KIBeldAVo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(bos.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-RTr19ffIGQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "34c7af7b-5b15-4be3-de36-6606b5ad268d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524779855060,
          "user_tz": 420,
          "elapsed": 441,
          "user": {
            "displayName": "PARTH RAVINDRA INGLE",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106483359159021417559"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = bos.drop('PRICE', axis = 1)\n",
        "Y = bos['PRICE']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
        "\n",
        "X_train = np.asarray(X_train).T\n",
        "X_test = np.asarray(X_test).T\n",
        "Y_train = np.asarray(Y_train).reshape(1, Y_train.shape[0])\n",
        "Y_test = np.asarray(Y_test).reshape(1, Y_test.shape[0])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12, 339)\n",
            "(12, 167)\n",
            "(1, 339)\n",
            "(1, 167)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FS4U139z9ad4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Architecture of the Learning Algorithm**\n",
        "\n",
        "Now it's time to get started on the learning algorithm itself! You will build a linear regression algorithm.\n",
        "\n",
        "\n",
        "**Mathematical algorithm:**\n",
        "\n",
        "Given $x^{i}$ which is 1 training example of size (12 , 1), \n",
        "\n",
        "$$h(x^{i})=z^{i} = w^{T}x^{i} + b$$\n",
        "\n",
        "$$Loss = L(y^{i}, a^{i})=(a^{i} - y^{i})^{2}$$\n",
        "\n",
        "Then to calculate the cost over all m training samples,\n",
        "\n",
        "$$Cost = J = \\frac{1}{2m}\\sum _{i = 1}^{m} L(y^{i}, a^{i})$$\n",
        "\n",
        "\n",
        "The superscript $^{T}$ just means transpose. What this algorithm is doing is calculating the model predictions for each training sample, and then finding the average loss over all the samples where loss is a measure of how far off the predictions are from the correct labels.\n",
        "\n",
        "**Next steps are:**\n",
        "\n",
        "\n",
        "1.   Define model structure\n",
        "2.   Initialize parameters\n",
        "3.   Implement gradient descent: calculate loss, gradient, and update parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wi6C7dMhdvPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initializing Parameters\n",
        "\n",
        "We need a function that inputs a matrix size, and then creates a weight matrix and a bias that are initialized to zero.\n",
        "\n",
        "Hint: `np.zeros(shape=(x,y))` returns a numpy array of zeros of shape (x,y)"
      ]
    },
    {
      "metadata": {
        "id": "_vmYtA35dxvh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 2 line of code)\n",
        "    w = None\n",
        "    b = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdNfsBnLd8vB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GuBNvOteMSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation\n",
        "\n",
        "Now it's time to implement forward propagation. During forward propagation we will compute the model predictions, the cost over all of our samples and the gradients for the weights and bias. Here are the steps we will need to take:\n",
        "\n",
        "1. use the equation $w^{T}X + b$ to compute $A$  *(the capital $X$ means that it is a matrix containing all of our $x^{i}$ samples)*\n",
        "3. use the cost function described earlier to compute our cost $J$\n",
        "4. compute the gradients using the equations given below\n",
        "\n",
        "As a reminder our cost function is $J = \\frac{1}{2m}\\sum _{i = 1}^{m} (a^{i} - y^{i})^{2}$\n",
        "\n",
        "**Gradients:**\n",
        "\n",
        "\n",
        "$$db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m}\\sum _{i = 1}^{m} a^{i} - y^{i}$$\n",
        "$$$$\n",
        "$$dw = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}\\sum _{i = 1}^{m} (a^{i} - y^{i})x^{i}$$\n",
        "\n",
        "If you have taken multivariable calculus then these equations are the partial derivatives of our cost function $J$ with respect to our weights $w$ and our bias $b$. By taking their negatives we can optimize our cost function in the direction of steepest descent. If the last two scentences made no sense then fear not. You can simply implement these equations without fully understanding them. Just know that using these equations we can reduce our cost and move closer to a loss of 0 (i.e. a correct prediction).\n",
        "$$$$\n",
        "\n",
        "Hint: \n",
        "* you can use `np.dot()` to multiply two matrices together\n",
        "* you can use `np.sum()` to take the sum of all the elements in a matrix"
      ]
    },
    {
      "metadata": {
        "id": "aNDNBv7VfDne",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (13, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (12, number of examples)\n",
        "    Y -- house prices (in 1000s)\n",
        "\n",
        "    Return:\n",
        "    cost -- mean squared error\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A = None # compute activation\n",
        "    cost =  None # compute cost\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dw = None\n",
        "    db = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWQ_9fpyfYPd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5hBl0vqfXER",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected Output:\n",
        "\n",
        "* dw = [[16.] [ 36.]]\n",
        "* db = 10.0\n",
        "* cost = 52.0"
      ]
    },
    {
      "metadata": {
        "id": "icm10Ef3gHkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "\n",
        "Now we want to use the gradients that we computed earlier to perform back propagation. Back propagation is process we use to update our weights bias by a small amount so that our model gets a little better better at making predictions. Here are the equations in case you forgot:\n",
        "\n",
        "$$w = w - \\alpha * dw$$\n",
        "$$b = b - \\alpha * db$$\n",
        "\n",
        "$\\alpha$ is our learning rate, which is often a small number like .01, .001, or .000001. If you remember from the slides, we want to update our weights and bias without overshooting the minimum for our cost function. The picture below illustrates this.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/aibootcamp/Week%201/assets/divergence.jpg\" width=\"350\" height=\"240\"/>"
      ]
    },
    {
      "metadata": {
        "id": "39ly_7vTgNzT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (12, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (12, number of examples)\n",
        "    Y -- house prices (in 1000s)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (≈ 1 line of code)\n",
        "        ### START CODE HERE ### \n",
        "        grads, cost = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = None\n",
        "        b = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i != 0 and i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i != 0 and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ivgLqBAgooY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIB36aWsgiYH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected Output:\n",
        "* **w = ** [[-0.08140605] [-0.24334521]]\n",
        "* **b = ** 1.419030420562572\n",
        "* **dw = ** [[ 0.08670598] [-0.02296021]]\n",
        "* **db = ** -0.05483309171519252"
      ]
    },
    {
      "metadata": {
        "id": "8SOFU5MTiCK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Predictions\n",
        "\n",
        "Once we have trained our model, we need a way to use it. For that we will create a function called predict that will take an input of training samples, a weight matrix and a bias, and then output predictions price of the house.\n",
        "\n",
        "$$A = w^{T}X + b$$"
      ]
    },
    {
      "metadata": {
        "id": "-0DPno_EiBBn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict the house price using learned linear regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (12, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (12, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # Compute vector \"A\" predicting the the house price\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    prediction = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(prediction.shape == (1, m))\n",
        "    \n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFRKiF1XimEk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOiVxBqmio0G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "* [[9 12]]"
      ]
    },
    {
      "metadata": {
        "id": "bbIDA11EjLXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Now that we all the pieces of linear regression, lets put all together into one function. \n",
        "\n",
        "Implement the model function. Use the following notation:\n",
        "\n",
        "* `Y_prediction` for your predictions on the test set\n",
        "* `Y_prediction_train` for your predictions on the train set\n",
        "* `w`, `costs`, `grads` for the outputs of optimize()"
      ]
    },
    {
      "metadata": {
        "id": "_DPxJbsUjTG3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.01, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the linear regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (12, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (12, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = None\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = None\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = None\n",
        "    b = None\n",
        "    \n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    prediction_test = None\n",
        "    prediction_train = None\n",
        "\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    #convert error to RMSE\n",
        "    train_rmse = np.sqrt( 1 / X_train.shape[1] * np.sum(( prediction_train - Y_train ) ** 2) )\n",
        "    test_rmse = np.sqrt( 1 / X_test.shape[1] * np.sum(( prediction_test - Y_test ) ** 2) )\n",
        "    \n",
        "    # Print train/test Errors\n",
        "    print(\"\\ntrain RMSE: {}\".format(train_rmse))\n",
        "    print(\"test RMSE: {}\".format(test_rmse))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": prediction_test, \n",
        "         \"Y_prediction_train\" : prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P57JZtZGkPyn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 500000, learning_rate = 0.00001, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmv7Zwl4v-Kq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected Output:\n",
        "* train RMSE: 6.0800950740323705\n",
        "* test RMSE: 6.491753239651829"
      ]
    },
    {
      "metadata": {
        "id": "6F99LMbTmjD8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per thousands)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}