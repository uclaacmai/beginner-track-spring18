{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to Neural Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "WTx54GpKLcXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Neural Networks\n",
        "This program will first implement a linear classifier and then extend the code to a neural network.\n",
        "## Importing Packages\n",
        "Just importing numpy and matplotlib.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PFL6PTMB4jOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Credits Stanford CS 231n\n",
        "# A bit of setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKtkP3B9MeJr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Plotting the data\n",
        "\n",
        "We are creating a \"spiral dataset\" with three classes, 100 points per class. Visual representation of the resulting dataset is shown below. Notice that the dataset is **not** easily linearly separable."
      ]
    },
    {
      "metadata": {
        "id": "aTv3rzuM4jOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "N = 100 # number of points per class\n",
        "D = 2 # dimensionality\n",
        "K = 3 # number of classes\n",
        "X = np.zeros((N*K,D))\n",
        "y = np.zeros(N*K, dtype='uint8')\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "  y[ix] = j\n",
        "fig = plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim([-1,1])\n",
        "plt.ylim([-1,1])\n",
        "#fig.savefig('spiral_raw.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0JSnZ7zMkum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training a Linear Classifier\n",
        "\n",
        "Here we randomly initialize parameters to small values clustered around 0. Parameters are the weight matrix W and bias vector b for each class.\n",
        "\n",
        "We set hyperparameters  of learning rate and regularization strength.\n",
        "\n",
        "##Gradient Descent\n",
        "\n",
        "Here, we iterate through all of our training examples. We evaluate \"class scores\", which are the outputs of our hypothesis function, computed through matrix multiplication. \n",
        "\n",
        "We also calculate the **loss function**, taking average cross entropy loss and regularization. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Gradient of the loss function with respect to the parameters is calculated. We now backpropagate into W and b through the matrix multiply operation.\n",
        "\n",
        "Finally, the parameters are updated in the negative gradient direction to decrease the loss."
      ]
    },
    {
      "metadata": {
        "id": "_gjuO1_f4jOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train a Linear Classifier\n",
        "\n",
        "# initialize parameters randomly\n",
        "W = 0.01 * np.random.randn(D,K)\n",
        "b = np.zeros((1,K))\n",
        "\n",
        "# some hyperparameters\n",
        "step_size = 1e-1\n",
        "reg = 0# 1e-3 # regularization strength\n",
        "\n",
        "# gradient descent loop\n",
        "num_examples = X.shape[0]\n",
        "for i in range(200):\n",
        "  \n",
        "  # evaluate class scores, [N x K]\n",
        "  \n",
        "  ###################################\n",
        "  # TODO\n",
        "  \n",
        "  scores = None\n",
        "  \n",
        "  # END TODO\n",
        "  ###################################\n",
        "  \n",
        "  # compute the class probabilities\n",
        "  exp_scores = np.exp(scores)\n",
        "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
        "  \n",
        "  # compute the loss: average cross-entropy loss and regularization\n",
        "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
        "  data_loss = np.sum(corect_logprobs)/num_examples\n",
        "  reg_loss = 0.5*reg*np.sum(W*W)\n",
        "  loss = data_loss + reg_loss\n",
        "  if i % 10 == 0:\n",
        "    print (\"iteration %d: loss %f\" % (i, loss))\n",
        "  \n",
        "  # compute the gradient on scores\n",
        "  dscores = probs\n",
        "  dscores[range(num_examples),y] -= 1\n",
        "  dscores /= num_examples\n",
        "  \n",
        "  ###################################################\n",
        "  #TODO\n",
        "  # backpropate the gradient to the parameters (W,b)\n",
        "  \n",
        "  dW = 0\n",
        "  db = 0\n",
        "  \n",
        "  #dW += reg*W # regularization gradient\n",
        "  \n",
        "  # perform a parameter update\n",
        "  W += 0\n",
        "  b += 0\n",
        "  \n",
        "  # END TODO\n",
        "  ###################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOASrfT132_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see now that we have converged after these 190 iterations. Now we will evaluate the accuracy on the training set:"
      ]
    },
    {
      "metadata": {
        "id": "UcObMdsu4jOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate training set accuracy\n",
        "scores = np.dot(X, W) + b\n",
        "predicted_class = np.argmax(scores, axis=1)\n",
        "print ('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tEtngxQM4DJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training accuracy was low because we used a linear classifier."
      ]
    },
    {
      "metadata": {
        "id": "W-v8Czlo4jOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot the resulting classifier\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "#fig.savefig('spiral_linear.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4AKyGLUU06Di",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training a Neural Network\n",
        "\n",
        "As shown, a linear classifier is inadequate for this problem. Thus, we now implement a neural network. Below, we add one hidden layer. We'll also need two sets of weights and layers for our first and second layers.\n",
        "\n",
        "The only change in the forward pass code is at the comment:\n",
        "\n",
        "```\n",
        "# evaluate class scores, [N x K]\n",
        "```\n",
        " where the hidden layer representation, then the scores based on the hidden layer, are computed.\n",
        " \n",
        " Everything else, such as loss, gradient computation, etc. is the same as before.\n",
        " \n",
        " Backpropagation changes, though. Backpropagating the second layer remains the same as the code for the Softmax classifier, except we replace X (raw data) with hidden_layer (output from hidden layer).\n",
        "\n",
        "However, we need to backpropagate through the hidden layer, too!  Its gradient is computed using dscores and W2. We also have to backpropagate through the ReLU, and the first layer weights and biases. We end up with dW, db, dW2, db2, which are the gradients we need to update our parameters."
      ]
    },
    {
      "metadata": {
        "id": "zR_mdZdi4jOp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize parameters randomly\n",
        "h = 100 # size of hidden layer\n",
        "W = 0.01 * np.random.randn(D,h)\n",
        "b = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,K)\n",
        "b2 = np.zeros((1,K))\n",
        "\n",
        "# some hyperparameters\n",
        "step_size = 1e-1\n",
        "reg = 1e-3 # regularization strength\n",
        "\n",
        "# gradient descent loop\n",
        "num_examples = X.shape[0]\n",
        "for i in range(10000):\n",
        "  \n",
        "  # evaluate class scores, [N x K]\n",
        "  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
        "  scores = np.dot(hidden_layer, W2) + b2\n",
        "  \n",
        "  # compute the class probabilities\n",
        "  exp_scores = np.exp(scores)\n",
        "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
        "  \n",
        "  # compute the loss: average cross-entropy loss and regularization\n",
        "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
        "  data_loss = np.sum(corect_logprobs)/num_examples\n",
        "  \n",
        "  # Note that regularization loss is a function of initial and hidden-layer weights\n",
        "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
        "  loss = data_loss + reg_loss\n",
        "  if i % 1000 == 0:\n",
        "    print (\"iteration %d: loss %f\" % (i, loss))\n",
        "  \n",
        "  # compute the gradient on scores\n",
        "  dscores = probs\n",
        "  dscores[range(num_examples),y] -= 1\n",
        "  dscores /= num_examples\n",
        "  \n",
        "  # backpropate the gradient to the parameters\n",
        "  # first backprop into parameters W2 and b2\n",
        "  dW2 = np.dot(hidden_layer.T, dscores)\n",
        "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
        "  # next backprop into hidden layer\n",
        "  dhidden = np.dot(dscores, W2.T)\n",
        "  # backprop the ReLU non-linearity\n",
        "  dhidden[hidden_layer <= 0] = 0\n",
        "  # finally into W,b\n",
        "  dW = np.dot(X.T, dhidden)\n",
        "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
        "  \n",
        "  # add regularization gradient contribution\n",
        "  dW2 += reg * W2\n",
        "  dW += reg * W\n",
        "  \n",
        "  # perform a parameter update\n",
        "  W += -step_size * dW\n",
        "  b += -step_size * db\n",
        "  W2 += -step_size * dW2\n",
        "  b2 += -step_size * db2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOFsu9S74jOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate training set accuracy\n",
        "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
        "scores = np.dot(hidden_layer, W2) + b2\n",
        "predicted_class = np.argmax(scores, axis=1)\n",
        "print ('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZByobna5_wp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Final results##\n",
        "Now our accuracy is much higher, and as seen above the code for a neural network isn't much more complex than the code for a linear classifier. We can also visualize the decision boundaries, as shown below."
      ]
    },
    {
      "metadata": {
        "id": "I_Z-93Vk4jOu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot the resulting classifier\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "#fig.savefig('spiral_net.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}